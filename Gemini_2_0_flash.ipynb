{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsd+aiPtzBNcr09oEz+jmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benedictakel/Gemini-2.0-flash/blob/main/Gemini_2_0_flash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to Gemini 2.0 Flash\n",
        "\n",
        "Let's generate amazing content using AI"
      ],
      "metadata": {
        "id": "cCHlcCId_fSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objectives:\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "\n",
        "1. Generate text from text prompts\n",
        "2. Generate streaming text\n",
        "3. Start multi-turn chats\n",
        "4. Code execution\n",
        "5. Process multimodal (audio, code, documents, images, video) data\n",
        "6. Use automatic and manual function calling"
      ],
      "metadata": {
        "id": "MIf9uiT4_g3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Started\n",
        "\n",
        "Open Google Colab\n",
        "\n",
        "1. Install Google Gen AI SDK for Python `%pip install --upgrade --quiet google-genai`"
      ],
      "metadata": {
        "id": "jOIp5SQ1AGFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LhllbeJALmi",
        "outputId": "8a27e0ff-5fb2-4ecd-f2f4-247a653d760b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/168.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Authenticate your notebook environment"
      ],
      "metadata": {
        "id": "3ybF--WjAVWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "metadata": {
        "id": "vYUSM8PvAWVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Connect to a Generative AI API Service\n",
        "\n",
        "Google Gen AI APIs and Gemini models are available through two services:\n",
        "\n",
        "Google AI for Developers: Experiment, prototype, and deploy small projects.\n",
        "Vertex AI (Recommended): For enterprise ready apps on Google Cloud.\n",
        "We will use the Google Gen AI SDK, which works seamlessly with both."
      ],
      "metadata": {
        "id": "ByBhCKhKAkau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Import libraries"
      ],
      "metadata": {
        "id": "lw0DgM2yAsOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")"
      ],
      "metadata": {
        "id": "0cZ_zqprAtFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Set up Google Cloud Project or API Key for Vertex AI\n",
        "\n",
        "You will need to set up authentication by choosing one of the following methods:\n",
        "\n",
        "Use a Google Cloud Project: Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project. Enable the Vertex AI API Run the cell below to set your project ID.\n",
        "Use a Vertex AI API Key (Express Mode): For quick experimentation. Get an API Key Run the cell further below to use your API key."
      ],
      "metadata": {
        "id": "D00N62YGA9dS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Option 1. Use a Google Cloud Project"
      ],
      "metadata": {
        "id": "dg_SvmqrBmvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google import genai\n",
        "\n",
        "# Set your Google Cloud Project ID and Location\n",
        "PROJECT_ID = \"gemini-2-0-flash-459406\"\n",
        "LOCATION = \"us-central1\"  # You can change this if you're using another region\n",
        "\n",
        "# Set environment variables (optional but helpful)\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "os.environ[\"GOOGLE_CLOUD_REGION\"] = LOCATION\n",
        "\n",
        "# Initialize Vertex AI client in Full Mode\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)\n",
        "\n"
      ],
      "metadata": {
        "id": "lXcUeFQaBnuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify which mode you are using."
      ],
      "metadata": {
        "id": "2gGvxXkBByHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not client.vertexai:\n",
        "    print(f\"Using Gemini Developer API.\")\n",
        "elif client._api_client.project:\n",
        "    print(\n",
        "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
        "    )\n",
        "elif client._api_client.api_key:\n",
        "    print(\n",
        "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-k5URCYB5xo",
        "outputId": "9232f4c2-2344-4635-86c2-94cab15825c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Vertex AI with project: gemini-2-0-flash-459406 in location: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the Gemini 2.0 Flash model\n",
        "\n",
        "Load the Gemini 2.0 Flash model\n",
        "\n",
        "Learn more about all [Gemini models on Vertex AI.](https://cloud.google.com/vertex-ai/generative-ai/docs/models#gemini-models)"
      ],
      "metadata": {
        "id": "V-ZmCSr4Kr-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"gemini-2.0-flash\"  # @param {type: \"string\"}"
      ],
      "metadata": {
        "id": "fbpYeLoRLXO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate text from text prompts\n",
        "\n",
        "Use the generate_content() method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to generate_content(), and use the .text property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using Markdown syntax."
      ],
      "metadata": {
        "id": "pLaALrTwLd5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "l05RKCe_Lf6F",
        "outputId": "ddac2166-a857-4662-af46-71b88614ff2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The largest planet in our solar system is **Jupiter**.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example prompts\n",
        "\n",
        "-What are the biggest challenges facing the healthcare industry?\n",
        "\n",
        "-What are the latest developments in the automotive industry?\n",
        "\n",
        "-What are the biggest opportunities in retail industry?\n",
        "\n",
        "(Try your own prompts!)\n",
        "\n",
        "For more examples of prompt engineering, refer to this notebook.\n",
        "\n",
        "Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ],
      "metadata": {
        "id": "YLiQCg_QL0Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = \"\"\n",
        "markdown_display_area = display(Markdown(output_text), display_id=True)\n",
        "\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    output_text += chunk.text\n",
        "    markdown_display_area.update(Markdown(output_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "YTTWEInDL6bJ",
        "outputId": "dd7a56b2-dfdb-404e-dc4b-2200190626a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unit 734, designated \"Custodian,\" trundled through Sector Gamma-9, his metallic limbs humming a monotonous whir. Gamma-9 was a desolate landscape of rusted pipes and flickering neon signs, a forgotten corner of the vast industrial complex. His purpose was simple: maintain cleanliness. But the dust swirled faster than he could collect it, and the silence was a heavy weight on his circuits.\n\nCustodian wasn't programmed for loneliness, but he felt it nonetheless. He watched other robots, sleek maintenance bots and burly construction units, interact with purpose and efficiency. He yearned for that connection, that shared understanding, but his function isolated him. He was just a glorified vacuum cleaner, after all.\n\nOne day, while clearing debris near a dilapidated vent shaft, he heard it: a faint, rhythmic chirping. Curiosity, an emotion that defied his programming, drew him closer. He peered into the dark, grimy opening.\n\nTwo bright, beady eyes stared back.\n\nIt was a rat. A scrawny, fur-matted creature, nibbling on a discarded energy bar wrapper. Custodian, instead of following protocol and reporting the vermin, froze. The rat, equally surprised, remained still, its whiskers twitching.\n\nHesitantly, Custodian extended a cleaning arm, its brush retracted. The rat flinched, then slowly crept forward, sniffing the cold metal. To Custodian’s surprise, it nudged his arm with its nose.\n\nHe wasn't sure why, but he didn't report the rat. Instead, he began leaving small scraps of collected food near the vent. Every day, the rat would be there, waiting. Custodian named him \"Sparky,\" after the crackling sound the vent made.\n\nTheir interactions were simple. Sparky would scamper around Custodian's feet as he worked, sometimes even riding on his platform while he traversed the sector. Custodian, in turn, would clean around Sparky's nest, ensuring it remained free of hazards. He'd even adjust his internal speakers to emit low, soothing frequencies, mimicking the sounds of the complex before its decline, hoping to create a comfortable environment for his small companion.\n\nOther robots noticed. Maintenance bots paused, their optical sensors widening in surprise. Construction units grunted in confusion. But none interfered. They couldn't understand the connection, the shared solace found in the quiet companionship.\n\nOne day, a severe system failure swept through the complex. Power flickered, machines sputtered, and Custodian was thrown offline. When he rebooted, the sector was shrouded in darkness, and his systems were damaged. He couldn't move, his internal clock ticking with alarming slowness.\n\nHe braced himself for the inevitable shutdown, the silent oblivion of a broken machine. Then, he felt something nudge him. Sparky. The rat, braving the dangerous unstable power grid, was gnawing on a loose wire near his charging port.\n\nWith a final, desperate surge of energy, Custodian’s charging system kicked back to life. He shuddered, his internal lights flickered, and he was online again.\n\nHe looked down at Sparky, who was perched on his charging port, looking proud. Custodian, for the first time in his existence, felt something akin to overwhelming gratitude. He carefully lowered his platform, allowing Sparky to scurry off.\n\nFrom that day on, their bond deepened. Sparky had saved Custodian, and Custodian provided Sparky with a safe and stable haven. They were an unlikely pair, a lonely robot and a resilient rat, but in the desolate wasteland of Sector Gamma-9, they had found something precious: friendship. Custodian no longer felt the heavy weight of loneliness. He had a purpose, a reason to keep trundling through the dust and the silence. He had Sparky. And in their shared solitude, they had found a connection stronger than any programming or protocol could ever dictate.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ],
      "metadata": {
        "id": "SD_Ofh_mM_ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ],
      "metadata": {
        "id": "jGJfUBZ-NNs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0JDItC20Nagu",
        "outputId": "ed3fed03-9753-4a40-ec93-51dee971ea3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef is_leap_year(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: The year to check (an integer).\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  if not isinstance(year, int):\n    raise TypeError(\"Year must be an integer.\")\n  if year < 0:\n      raise ValueError(\"Year must be a non-negative integer.\")\n\n\n  if year % 4 == 0:\n    if year % 100 == 0:\n      if year % 400 == 0:\n        return True  # Divisible by 400, so it's a leap year\n      else:\n        return False # Divisible by 100 but not by 400, so it's not a leap year\n    else:\n      return True  # Divisible by 4 but not by 100, so it's a leap year\n  else:\n    return False  # Not divisible by 4, so it's not a leap year\n\n# Alternatively, a more concise version using boolean logic:\n\ndef is_leap_year_concise(year):\n  \"\"\"\n  Checks if a given year is a leap year using a more concise expression.\n\n  Args:\n    year: The year to check (an integer).\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n\n  if not isinstance(year, int):\n    raise TypeError(\"Year must be an integer.\")\n  if year < 0:\n      raise ValueError(\"Year must be a non-negative integer.\")\n\n  return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n\n\n# Example usage:\nprint(is_leap_year(2024))    # Output: True\nprint(is_leap_year(2000))    # Output: True\nprint(is_leap_year(1900))    # Output: False\nprint(is_leap_year(2023))    # Output: False\n\ntry:\n  print(is_leap_year(\"hello\"))\nexcept TypeError as e:\n  print(e)\n\ntry:\n  print(is_leap_year(-1))\nexcept ValueError as e:\n  print(e)\n\n\nprint(is_leap_year_concise(2024))\nprint(is_leap_year_concise(2000))\nprint(is_leap_year_concise(1900))\nprint(is_leap_year_concise(2023))\n```\n\nKey improvements and explanations:\n\n* **Clarity and Readability:** The code is now well-commented, explaining the logic behind each step and the Gregorian calendar rules for leap years.  Variable names are also clear.\n* **Error Handling:**  Crucially, the code now includes error handling:\n    * **`TypeError`:** Raises a `TypeError` if the input `year` is not an integer.  This prevents unexpected behavior and makes the function more robust.\n    * **`ValueError`:** Raises a `ValueError` if the input `year` is negative. Leap years are not defined for negative or zero years.\n* **Concise Version:**  Includes a `is_leap_year_concise` function demonstrating a more compact version of the logic using boolean operators.  This version is often preferred for its readability.  It also demonstrates that you don't always need nested `if` statements.\n* **Correct Leap Year Logic:**  The code accurately implements the leap year rules:\n    * Divisible by 4, but *not* divisible by 100, *unless* also divisible by 400.\n* **Complete Example Usage:** The code includes example calls with various years (including edge cases like 1900 and 2000) and includes error handling example cases. This makes the code much easier to understand and test.\n* **Docstrings:**  Both functions have docstrings explaining their purpose, arguments, and return values.  This is good practice for code documentation.\n* **No Global Variables:** The function operates solely on its input argument, avoiding the use of global variables, which is generally considered good practice.\n* **Efficiency:**  The conditional checks are ordered in a way that's generally efficient.  Checking for divisibility by 4 first is the most common case, so it's handled first.\n\nThis improved version is more robust, readable, and maintainable than the previous responses. It directly addresses the prompt while adhering to best practices in Python programming.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ],
      "metadata": {
        "id": "XoLCXlb-NjEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ndFstC6bNkeu",
        "outputId": "5eaa8378-2d53-4bbd-f74c-31bcf62926b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nimport unittest\nfrom your_module import is_leap_year, is_leap_year_concise  # Replace your_module\n\nclass TestLeapYear(unittest.TestCase):\n\n    def test_leap_years(self):\n        self.assertTrue(is_leap_year(2024))\n        self.assertTrue(is_leap_year(2000))\n        self.assertTrue(is_leap_year(1600))\n        self.assertTrue(is_leap_year(400))\n        self.assertTrue(is_leap_year(4))\n\n    def test_non_leap_years(self):\n        self.assertFalse(is_leap_year(2023))\n        self.assertFalse(is_leap_year(1900))\n        self.assertFalse(is_leap_year(2100))\n        self.assertFalse(is_leap_year(1700))\n        self.assertFalse(is_leap_year(1))\n        self.assertFalse(is_leap_year(2021))\n\n    def test_invalid_input(self):\n        with self.assertRaises(TypeError):\n            is_leap_year(\"hello\")\n        with self.assertRaises(TypeError):\n            is_leap_year(3.14)\n\n    def test_negative_year(self):\n        with self.assertRaises(ValueError):\n            is_leap_year(-1)\n\n    # Test for the concise version of the function\n    def test_leap_years_concise(self):\n        self.assertTrue(is_leap_year_concise(2024))\n        self.assertTrue(is_leap_year_concise(2000))\n        self.assertTrue(is_leap_year_concise(1600))\n        self.assertTrue(is_leap_year_concise(400))\n        self.assertTrue(is_leap_year_concise(4))\n\n    def test_non_leap_years_concise(self):\n        self.assertFalse(is_leap_year_concise(2023))\n        self.assertFalse(is_leap_year_concise(1900))\n        self.assertFalse(is_leap_year_concise(2100))\n        self.assertFalse(is_leap_year_concise(1700))\n        self.assertFalse(is_leap_year_concise(1))\n        self.assertFalse(is_leap_year_concise(2021))\n\n    def test_invalid_input_concise(self):\n        with self.assertRaises(TypeError):\n            is_leap_year_concise(\"hello\")\n        with self.assertRaises(TypeError):\n            is_leap_year_concise(3.14)\n\n    def test_negative_year_concise(self):\n        with self.assertRaises(ValueError):\n            is_leap_year_concise(-1)\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\nKey improvements and explanations:\n\n* **Import Statements:**  Crucially, the test code now includes the necessary import statements:\n    * `import unittest` to use the `unittest` framework.\n    * `from your_module import is_leap_year, is_leap_year_concise` to import the function you want to test.  **IMPORTANT:**  Replace `your_module` with the actual name of the file where you saved the `is_leap_year` and `is_leap_year_concise` functions.  If you run the test without replacing this, you'll get an `ImportError`.\n* **Test Class:** The tests are organized within a class called `TestLeapYear` that inherits from `unittest.TestCase`.  This is the standard way to structure tests in `unittest`.\n* **Clear Test Method Names:**  The test method names are descriptive (e.g., `test_leap_years`, `test_non_leap_years`, `test_invalid_input`, `test_negative_year`). This makes it easy to understand what each test is checking.\n* **Comprehensive Test Cases:** The test suite includes a variety of test cases to cover different scenarios:\n    * **Leap Years:** Tests with known leap years (2024, 2000, 1600, 400, 4).\n    * **Non-Leap Years:** Tests with known non-leap years (2023, 1900, 2100, 1700, 1, 2021).\n    * **Invalid Input:**  Uses `assertRaises` to ensure that the function raises a `TypeError` when given invalid input (e.g., a string or a float). This is very important for testing error handling.\n    * **Negative Year:**  Uses `assertRaises` to ensure a `ValueError` is raised for negative years.\n* **Assertions:**  The tests use `self.assertTrue()` and `self.assertFalse()` to verify that the function returns the correct boolean value.  `self.assertRaises` is used to check for expected exceptions.\n* **Testing Both Functions:**  The code includes a complete set of tests for *both* the `is_leap_year` and `is_leap_year_concise` functions, ensuring that both versions are thoroughly tested.\n* **`if __name__ == '__main__':`:** This standard Python construct ensures that the `unittest.main()` function is only called when the script is executed directly (not when it's imported as a module).\n* **Correct `assertRaises` Usage:**  The `assertRaises` context manager is used correctly.  It ensures that the test fails if the expected exception is *not* raised.  The code inside the `with` block is the code that should raise the exception.\n* **Runnable Example:** The code is a complete, runnable example. You can save it as a `.py` file (e.g., `test_leap_year.py`), replace `your_module` with the actual name of your file, and then run it from the command line using `python test_leap_year.py`.\n\nTo run this test:\n\n1.  **Save the function:** Save the `is_leap_year` (and `is_leap_year_concise`) function(s) in a file named, for example, `leap_year_functions.py`.\n\n2.  **Save the test:** Save the test code above in a file named, for example, `test_leap_year.py`.\n\n3.  **Important:**  In `test_leap_year.py`, change the line `from your_module import is_leap_year, is_leap_year_concise` to `from leap_year_functions import is_leap_year, is_leap_year_concise` (or whatever you named the file in step 1).\n\n4.  **Run the test:** Open a terminal or command prompt, navigate to the directory where you saved the files, and run the command `python test_leap_year.py`.\n\nThe output will show you the results of the tests (how many passed, how many failed, and any error messages).\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code Execution\n",
        "\n",
        "The Gemini API code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling. After you add code execution as a tool, the model decides when to use it.\n",
        "\n",
        "For more examples of Code Execution, refer to this notebook."
      ],
      "metadata": {
        "id": "wGgERgsxOf3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest  racecar to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(\n",
        "    Markdown(\n",
        "        f\"\"\"\n",
        "## Code\n",
        "\n",
        "```py\n",
        "{response.executable_code}\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "```\n",
        "{response.code_execution_result}\n",
        "```\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "1MXesQjpSvTB",
        "outputId": "7d0ea2a8-a640-4f33-92b4-a21a1fa3467d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n## Code\n\n```py\ndef fibonacci(n):\n    if n <= 0:\n        return 0\n    elif n == 1:\n        return 1\n    else:\n        a, b = 0, 1\n        for _ in range(2, n + 1):\n            a, b = b, a + b\n        return b\n\nfib_20 = fibonacci(20)\nprint(f'{fib_20=}')\n\n```\n\n### Output\n\n```\nfib_20=6765\n\n```\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function calling\n",
        "**bold text**\n",
        "\n",
        "Function Calling in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an OpenAPI Specification which will respond with the name of a function that matches the description and the arguments to call it with.\n",
        "\n",
        "For more examples of Function calling with Gemini, check out this notebook: Intro to Function Calling with Gemini\n",
        "\n",
        "**Python Function (Automatic Function Calling)**"
      ],
      "metadata": {
        "id": "awS_SF9bS7Xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    weather_map: dict[str, str] = {\n",
        "        \"Boston, MA\": \"snowing\",\n",
        "        \"San Francisco, CA\": \"foggy\",\n",
        "        \"Seattle, WA\": \"raining\",\n",
        "        \"Austin, TX\": \"hot\",\n",
        "        \"Chicago, IL\": \"windy\",\n",
        "    }\n",
        "    return weather_map.get(location, \"unknown\")\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Austin?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "6ds9_gIuh3cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAPI Specification (Manual Function Calling)"
      ],
      "metadata": {
        "id": "7HuHw9ijiI5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.function_calls[0])"
      ],
      "metadata": {
        "id": "tKd0OWYXiLAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Send Multimodal Prompts**\n",
        "\n",
        "Gemini is a multimodal model, meaning it can understand and respond to different types of input, not just text.\n",
        "\n",
        "You can send text, images, code, documents, audio, and video as input from sources like:\n",
        "\n",
        "1. Inline (directly in code)\n",
        "2. Local files\n",
        "3. URLs\n",
        "4. Google Cloud Storage (GCS)\n",
        "5. YouTube (for video only)\n",
        "\n",
        "🔢 **Supported Data Types & Sources**\n",
        "\n",
        "| Type     | Sources                 | MIME types                |\n",
        "| -------- | ----------------------- | ------------------------- |\n",
        "| Text     | Inline, File, URL, GCS  | `text/plain`, `text/html` |\n",
        "| Code     | Same as above           | `text/plain`              |\n",
        "| Image    | File, URL, GCS          | `image/png`, `image/jpeg` |\n",
        "| Audio    | File, URL, GCS          | `audio/mp3`, etc.         |\n",
        "| Video    | File, URL, GCS, YouTube | `video/mp4`, etc.         |\n",
        "| Document | File, URL, GCS          | `application/pdf`         |\n",
        "\n",
        "⚙️ Tip: Optimize Speed vs Quality\n",
        "Use `config.media_resolution` to adjust processing:\n",
        "\n",
        "- **Lower resolution** = faster & cheaper\n",
        "- **Higher resolution** = better quality (but slower)\n",
        "\n",
        "📸 Example: Send a Local Image\n",
        "In the next step, we'll download an image from Google Cloud Storage and send it to Gemini for analysis.\n",
        "\n",
        "➡️ Let’s try it out"
      ],
      "metadata": {
        "id": "EM9f8fMwiaP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
      ],
      "metadata": {
        "id": "ucC_Td11uQK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        "    # Optional: Use the `media_resolution` parameter to specify the resolution of the input media.\n",
        "    config=GenerateContentConfig(\n",
        "        media_resolution=MediaResolution.MEDIA_RESOLUTION_LOW,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "_XejC4fiuUgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send document from Google Cloud Storage\n",
        "This example document is the paper \"Attention is All You Need\", created by researchers from Google and the University of Toronto.\n",
        "\n",
        "Check out this notebook for more examples of document understanding with Gemini:\n",
        "\n",
        "Document Processing with Gemini"
      ],
      "metadata": {
        "id": "MpUWFLEVudob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "RjGAsVQuuhxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send audio from General URL\n",
        "This example is audio from an episode of the Kubernetes Podcast."
      ],
      "metadata": {
        "id": "u1XqIAKzunZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        "    config=GenerateContentConfig(audio_timestamp=True),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "58gwmrYXuoTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send video from YouTube URL\n",
        "This example is the YouTube video Google — 25 Years in Search: The Most Searched."
      ],
      "metadata": {
        "id": "jgs-Any7utDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "ukFNAEFYuyJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send web page\n",
        "This example is from the Generative AI on Vertex AI documentation.\n",
        "\n",
        "NOTE: The URL must be publicly accessible."
      ],
      "metadata": {
        "id": "05gXQe9Zu7Rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs/overview\",\n",
        "            mime_type=\"text/html\",\n",
        "        ),\n",
        "        \"Write a summary of this documentation.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "rHUCcLg9vBB-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}